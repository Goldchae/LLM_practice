# 9.2ì ˆ LLM ìºì‹œ
# ì˜ˆì œ 9.6 ì‹¤ìŠµì— ì‚¬ìš©í•  OpenAIì™€ í¬ë¡œë§ˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
import os
import chromadb
from openai import OpenAI

api_key = os.environ["OPENAI_API_KEY"] = "ìžì‹ ì˜ OpenAI API í‚¤ ìž…ë ¥"

openai_client = OpenAI(api_key = api_key)
chroma_client = chromadb.Client()


"""
ðŸ’´ LLM ìºì‹œ
LLM ìƒì„±(ì¶”ë¡ ) => ë§Žì€ ì‹œê°„ê³¼ ë¹„ìš©

ê°€ëŠ¥í•˜ë©´ ìµœëŒ€í•œ ì¤„ì—¬ì•¼ í•¨

 

ìš”ì²­ê³¼ ìƒì„± ê²°ê³¼ë¥¼ ê¸°ë¡í•˜ê³  ì´í›„ì— ë™ì¼í•˜ê±°ë‚˜ ë¹„ìŠ·í•œ ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ ìƒˆë¡­ê²Œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•Šê³  ì´ì „ì˜ ìƒì„± ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ ì‘ë‹µí•¨ìœ¼ë¡œì¨ LLM ì‘ë‹µ ì‹œê°„ì„ ì¤„ìž„ 

 

 

ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ìš©í•´ì„œ LLM ìºì‹œ ì§ì ‘ êµ¬í˜„í•´ ë³´ê¸°

ðŸ’¶ LLM ìºì‹œ ìž‘ë™ ì›ë¦¬
ìºì‹œ ìš”ì²­ì„ í†µí•´ ê¸°ì¡´ì— ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ìš”ì²­ì´ ìžˆì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ë§Œì•½ ìžˆì—ˆë‹¤ë©´ LLM ìºì‹œì— ì €ìž¥ëœ ë‹µë³€ì„ ëª¨ë¸ì— ì „ë‹¬

 

ðŸ’µ ì¼ì¹˜ ìºì‹œ (Exact match)

ë¬¸ìžì—´ì´ ì™„ì „ížˆ ë™ì¼í•œ ì‘ë‹µì— ëŒ€í•œ ìºì‹œ ì‚¬ìš©

ë”•ì…”ë„ˆë¦¬ í˜•ì‹ ìžë£Œêµ¬ì¡°ì—ì„œ í‚¤ê°€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì•„ ê°’ì„ ë°˜í™˜

 

ðŸ’µ ìœ ì‚¬ ê²€ìƒ‰ ìºì‹œ (Similar search)

ë¬¸ìžì—´ì´ ìœ ì‚¬í•œ ì‘ë‹µì— ëŒ€í•œ ìºì‹œ ì‚¬ìš©

 

ìš”ì²­ì„ ìž„ë² ë”© ëª¨ë¸ì„ ì´ìš©í•´ ìž„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜,

ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ìœ ì‚¬í•œ ìš”ì²­ì´ ìžˆëŠ”ì§€ ê²€ìƒ‰,

ìœ ì‚¬í•œ ë²¡í„°ê°€ ìžˆë‹¤ë©´ ë°˜í™˜,

ì—†ë‹¤ë©´ LLM ìš”ì²­ ìƒì„±í•˜ê³  ë²¡í„° DBì— ìƒˆë¡­ê²Œ ì €ìž¥

ðŸ¥ž ì‹¤ìŠµ : OpenAi api cache êµ¬í˜„
Chroma : ì˜¤í”ˆ ì†ŒìŠ¤ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤

 

time.time()
ì†Œìš” ì‹œê°„ ì²´í¬í•˜ê¸°
"""





# ì˜ˆì œ 9.7 LLM ìºì‹œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ì„ ë•Œ ë™ì¼í•œ ìš”ì²­ ì²˜ë¦¬ì— ê±¸ë¦° ì‹œê°„ í™•ì¸
import time

def response_text(openai_resp):
    return openai_resp.choices[0].message.content

question = "ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?"
for _ in range(2):
    start_time = time.time()
    response = openai_client.chat.completions.create(
      model='gpt-3.5-turbo',
      messages=[
        {
            'role': 'user',
            'content': question
        }
      ],
    )
    response = response_text(response)
    print(f'ì§ˆë¬¸: {question}')
    print("ì†Œìš” ì‹œê°„: {:.2f}s".format(time.time() - start_time))
    print(f'ë‹µë³€: {response}\n')

# ì§ˆë¬¸: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?
# ì†Œìš” ì‹œê°„: 2.71s
# ë‹µë³€: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€ ê²¨ìš¸ ì‹œì¦Œì¸ 11ì›”ë¶€í„° ë‹¤ìŒ í•´ 3ì›”ê¹Œì§€ìž…ë‹ˆë‹¤. ì´ ê¸°ê°„ ë™ì•ˆ ê¸°ì˜¨ì´ ê¸‰ê²©ížˆ í•˜ë½í•˜ë©° í•œë°˜ë„ì— í•œê¸°ê°€ ë°€ë ¤ì˜¤ê²Œ ë©ë‹ˆë‹¤.

# ì§ˆë¬¸: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?
# ì†Œìš” ì‹œê°„: 4.13s
# ë‹µë³€: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì€ ê²¨ìš¸ì— ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê²ƒì´ ì¼ë°˜ì ìž…ë‹ˆë‹¤. ì´ ê¸°ë‹¨ë“¤ì€ ì£¼ë¡œ 11ì›”ë¶€í„° 2ì›”ì´ë‚˜ 3ì›”ê¹Œì§€ êµ­ë‚´ì— ì˜í–¥ì„ ë¯¸ì¹˜ë©°, í•œêµ­ì˜ ê²¨ìš¸ì²  ì¶”ìœ„ì™€ í•¨ê»˜ í•œë°˜ë„ ì „ì—­ì— í˜•ì„±ë˜ëŠ” ê°•í•œ ì„œë¶í’ê³¼ ëƒ‰ê¸°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.









# ì˜ˆì œ 9.8 íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¥¼ í™œìš©í•œ ì¼ì¹˜ ìºì‹œ êµ¬í˜„
class OpenAICache:
    def __init__(self, openai_client):
        self.openai_client = openai_client
        self.cache = {}

    def generate(self, prompt):
        if prompt not in self.cache:
            response = self.openai_client.chat.completions.create(
                model='gpt-3.5-turbo',
                messages=[
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ],
            )
            self.cache[prompt] = response_text(response)
        return self.cache[prompt]

openai_cache = OpenAICache(openai_client)

question = "ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?"
for _ in range(2):
    start_time = time.time()
    response = openai_cache.generate(question)
    print(f'ì§ˆë¬¸: {question}')
    print("ì†Œìš” ì‹œê°„: {:.2f}s".format(time.time() - start_time))
    print(f'ë‹µë³€: {response}\n')

# ì§ˆë¬¸: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?
# ì†Œìš” ì‹œê°„: 2.74s
# ë‹µë³€: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€ ê²¨ìš¸ ì‹œì¦Œì¸ 11ì›”ë¶€í„° ë‹¤ìŒí•´ 4ì›”ê¹Œì§€ìž…ë‹ˆë‹¤. ì´ ê¸°ê°„ ë™ì•ˆ ê¸°ë‹¨ì˜ ì˜í–¥ìœ¼ë¡œ í•œë°˜ë„ì—ëŠ” ì¶”ìš´ ë‚ ì”¨ì™€ í•¨ê»˜ ê°•í•œ ë°”ëžŒì´ ë¶ˆê²Œ ë˜ë©°, ëŒ€ì²´ë¡œ í•œë°˜ë„ì˜ ê²¨ìš¸ì²  ê¸°ì˜¨ì€ ë§¤ìš° ë‚®ì•„ì§‘ë‹ˆë‹¤.

# ì§ˆë¬¸: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?
# ì†Œìš” ì‹œê°„: 0.00s
# ë‹µë³€: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€ ê²¨ìš¸ ì‹œì¦Œì¸ 11ì›”ë¶€í„° ë‹¤ìŒí•´ 4ì›”ê¹Œì§€ìž…ë‹ˆë‹¤. ì´ ê¸°ê°„ ë™ì•ˆ ê¸°ë‹¨ì˜ ì˜í–¥ìœ¼ë¡œ í•œë°˜ë„ì—ëŠ” ì¶”ìš´ ë‚ ì”¨ì™€ í•¨ê»˜ ê°•í•œ ë°”ëžŒì´ ë¶ˆê²Œ ë˜ë©°, ëŒ€ì²´ë¡œ í•œë°˜ë„ì˜ ê²¨ìš¸ì²  ê¸°ì˜¨ì€ ë§¤ìš° ë‚®ì•„ì§‘ë‹ˆë‹¤.







#  ì˜ˆì œ 9.9 ìœ ì‚¬ ê²€ìƒ‰ ìºì‹œ ì¶”ê°€ êµ¬í˜„
class OpenAICache:
    def __init__(self, openai_client, semantic_cache):
        self.openai_client = openai_client
        self.cache = {}
        self.semantic_cache = semantic_cache

    def generate(self, prompt):
        if prompt not in self.cache:
            similar_doc = self.semantic_cache.query(query_texts=[prompt], n_results=1)
            if len(similar_doc['distances'][0]) > 0 and similar_doc['distances'][0][0] < 0.2:
                return similar_doc['metadatas'][0][0]['response']
            else:
                response = self.openai_client.chat.completions.create(
                    model='gpt-3.5-turbo',
                    messages=[
                        {
                            'role': 'user',
                            'content': prompt
                        }
                    ],
                )
                self.cache[prompt] = response_text(response)
                self.semantic_cache.add(documents=[prompt], metadatas=[{"response":response_text(response)}], ids=[prompt])
        return self.cache[prompt]
"""
semantic_cache : ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸

 

1. ì¼ì¹˜ ìºì‹œ ìžˆëŠ”ì§€ í™•ì¸/ë°˜í™˜

2. ì—†ìœ¼ë©´ í¬ë¡œë§ˆ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ query ë©”ì„œë“œì— query_texts ìž…ë ¥ (ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë“±ë¡ëœ ìž„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ë¥¼ ìž„ë² ë”© ë²¡í„°ë¡œ ì „í™˜ / ê²€ìƒ‰ ìˆ˜í–‰ )

3. ê²€ìƒ‰ ê²°ê³¼ê°€ ì¡´ìž¬?/ ê²€ìƒ‰ ê²°ê³¼ì™€ ê²€ìƒ‰ ë°ì´í„° ì‚¬ì´ì˜ ê±°ë¦¬ê°€ ê°€ê¹Œìš´ì§€?(ìœ ì‚¬ë„) ì²´í¬/ë°˜í™˜

4. ë‹¤ ì—†ìœ¼ë©´ LLMì— ìƒˆë¡œ ìš”ì²­ , ì¼ì¹˜ ìºì‹œì™€ ìœ ì‚¬ ê²€ìƒ‰ ìºì‹œì— ì €ìž¥

 

chromaDBì˜ OpenAIEmbedding Fuction í´ëž˜ìŠ¤ì— api í‚¤ì™€ ìž„ë² ë”© ëª¨ë¸ ì´ë¦„ ìž…ë ¥.
"""





# ì˜ˆì œ 9.10 ìœ ì‚¬ ê²€ìƒ‰ ìºì‹œ ê²°ê³¼ í™•ì¸
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction
openai_ef = OpenAIEmbeddingFunction(
                api_key=os.environ["OPENAI_API_KEY"],
                model_name="text-embedding-ada-002"
            )

semantic_cache = chroma_client.create_collection(name="semantic_cache",
                  embedding_function=openai_ef, metadata={"hnsw:space": "cosine"})

openai_cache = OpenAICache(openai_client, semantic_cache)

questions = ["ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?",
            "ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?",
            "ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ í•œë°˜ë„ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?",
             "êµ­ë‚´ì— ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ í•¨ê»˜ ë¨¸ë¬´ë¦¬ëŠ” ê¸°ê°„ì€?"]
for question in questions:
    start_time = time.time()
    response = openai_cache.generate(question)
    print(f'ì§ˆë¬¸: {question}')
    print("ì†Œìš” ì‹œê°„: {:.2f}s".format(time.time() - start_time))
    print(f'ë‹µë³€: {response}\n')

# ì§ˆë¬¸: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?
# ì†Œìš” ì‹œê°„: 3.49s
# ë‹µë³€: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€ ê²¨ìš¸ì² ì¸ 11ì›”ë¶€í„° 3ì›” ë˜ëŠ” 4ì›”ê¹Œì§€ìž…ë‹ˆë‹¤. ...

# ì§ˆë¬¸: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?
# ì†Œìš” ì‹œê°„: 0.00s
# ë‹µë³€: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€ ê²¨ìš¸ì² ì¸ 11ì›”ë¶€í„° 3ì›” ë˜ëŠ” 4ì›”ê¹Œì§€ìž…ë‹ˆë‹¤. ...

# ì§ˆë¬¸: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ í•œë°˜ë„ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?
# ì†Œìš” ì‹œê°„: 0.13s
# ë‹µë³€: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€ ê²¨ìš¸ì² ì¸ 11ì›”ë¶€í„° 3ì›” ë˜ëŠ” 4ì›”ê¹Œì§€ìž…ë‹ˆë‹¤. ...

# ì§ˆë¬¸: êµ­ë‚´ì— ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ í•¨ê»˜ ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?
# ì†Œìš” ì‹œê°„: 0.11s
# ë‹µë³€: ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€ ê²¨ìš¸ì² ì¸ 11ì›”ë¶€í„° 3ì›” ë˜ëŠ” 4ì›”ê¹Œì§€ìž…ë‹ˆë‹¤. ...