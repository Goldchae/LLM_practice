# 9.4ì ˆ ë°ì´í„° ë¡œê¹…
# ì˜ˆì œ 9.15 W&Bì— ë¡œê·¸ì¸í•˜ê¸°
import os
import wandb

wandb.login()
wandb.init(project="trace-example")

"""

ğŸ’´ ë°ì´í„° ë¡œê¹…
ì‚¬ìš©ìì˜ ì…ë ¥ê³¼ LLM ìƒì„± ì¶œë ¥ì„ ê¸°ë¡í•˜ëŠ” ë°ì´í„° ë¡œê¹…

 

ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ì„œë„ LLMì˜ ì‘ë‹µì´ ë§¤ë²ˆ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— 

ê¸°ë¡ í•„ìš” 

 

 

ğŸ—’ï¸ ëŒ€í‘œì ì¸ ë¡œê¹… ë„êµ¬ğŸ—’ï¸

- W&B

 
Weights & Biases: The AI Developer Platform

Weights & BiasesëŠ” ëª¨ë¸ì„ í•™ìŠµ ë° íŒŒì¸íŠœë‹í•˜ê³ , ì‹¤í—˜ë¶€í„° ìƒì‚°ê¹Œì§€ ëª¨ë¸ì„ ê´€ë¦¬í•˜ë©°, LLMìœ¼ë¡œ êµ¬ë™ë˜ëŠ” GenAI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¶”ì  ë° í‰ê°€í•  ìˆ˜ ìˆëŠ” ì„ ë„ì ì¸ AI ê°œë°œì í”Œë«í¼ì…ë‹ˆë‹¤.

site.wandb.ai
- MLflow

 
MLflow | MLflow

Description will go into a meta tag in <head />

mlflow.org
- PromptLayer

 
PromptLayer - The cleanest way to prompt engineer. Platform for prompt management, prompt evaluations, and LLM observability

Best practices Prompt Management and collaboration using a CMS Mar 7, 2024

www.promptlayer.com
 

 

 

ğŸ—’ï¸ ë¡œê¹… ë„êµ¬ W&B ì¨ë³´ê¸°

LLM ì…ë ¥, ì¶œë ¥, ì‹œê°„, ì—ëŸ¬ ìœ ë¬´, ë“± ì•Œ ìˆ˜ ìˆìŒ 

https://wandb.ai/authorize

api í‚¤ ì°¾ê¸°
"""





# ì˜ˆì œ 9.16 OpenAI API ë¡œê¹…í•˜ê¸°
import datetime
from openai import OpenAI
from wandb.sdk.data_types.trace_tree import Trace

api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

system_message = "You are a helpful assistant."
query = "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?"
temperature = 0.2
model_name = "gpt-3.5-turbo"

response = client.chat.completions.create(model=model_name,
                                        messages=[{"role": "system", "content": system_message},{"role": "user", "content": query}],
                                        temperature=temperature
                                        )

root_span = Trace(
      name="root_span",
      kind="llm",
      status_code="success",
      status_message=None,
      metadata={"temperature": temperature,
                "token_usage": dict(response.usage),
                "model_name": model_name},
      inputs={"system_prompt": system_message, "query": query},
      outputs={"response": response.choices[0].message.content},
      )

root_span.log(name="openai_trace")
'''
Trace í´ë˜ìŠ¤ : 

openAI ìš”ì²­ì˜ ì…ë ¥ê³¼ ìƒì„± ê²°ê³¼, ìƒì„± ì„±ê³µ ì—¬ë¶€, ìƒì„±ì— ì‚¬ìš©í•œ ì„¤ì •ê°’ ë“±ì„ ì´ìš©í•´ ê¸°ë¡í•  ë°ì´í„° ìƒì„±

 

Trace í´ë˜ìŠ¤ì˜ log ë©”ì„œë“œ :

ë¡œê·¸ë¥¼ W&Bì— ì €ì¥ (ê° ìš”ì²­ êµ¬ë¶„ì„ ìœ„í•œ ì´ë¦„ ì„¤ì •ë„ ê°€ëŠ¥)
'''









# ì˜ˆì œ 9.17 ë¼ë§ˆì¸ë±ìŠ¤ W&B ë¡œê¹…
from datasets import load_dataset
import llama_index
from llama_index.core import Document, VectorStoreIndex, ServiceContext
from llama_index.llms.openai import OpenAI
from llama_index.core import set_global_handler
# ë¡œê¹…ì„ ìœ„í•œ ì„¤ì • ì¶”ê°€
llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
set_global_handler("wandb", run_args={"project": "llamaindex"})
wandb_callback = llama_index.core.global_handler
service_context = ServiceContext.from_defaults(llm=llm)

dataset = load_dataset('klue', 'mrc', split='train')
text_list = dataset[:100]['context']
documents = [Document(text=t) for t in text_list]

index = VectorStoreIndex.from_documents(documents, service_context=service_context)

print(dataset[0]['question']) # ë¶íƒœí‰ì–‘ ê¸°ë‹¨ê³¼ ì˜¤í˜¸ì¸ í¬í•´ ê¸°ë‹¨ì´ ë§Œë‚˜ êµ­ë‚´ì— ë¨¸ë¬´ë¥´ëŠ” ê¸°ê°„ì€?

query_engine = index.as_query_engine(similarity_top_k=1, verbose=True)
response = query_engine.query(
    dataset[0]['question']
)
'''
set_global_handler í•¨ìˆ˜ :

ë¼ë§ˆì¸ë±ìŠ¤ ë‚´ë¶€ì—ì„œ W&Bì— ë¡œê·¸ë¥¼ ì „ì†¡í•˜ë„ë¡ ì„¤ì •
'''
